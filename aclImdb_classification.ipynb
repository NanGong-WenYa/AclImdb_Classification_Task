{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efeeb918-32b5-4695-b9fa-3c3d8dfaee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ad28ac-7d28-4375-b76d-a92f1f71db3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': ['This movie frequently extrapolates quantum mechanics to justify nonsensical ideas, capped by such statements like \"we all create our own reality\".<br /><br />Sorry, folks, reality is what true for all of us, not just the credulous.<br /><br />The idea that \"anything\\'s possible\" doesn\\'t hold water on closer examination: if anything\\'s possible, contrary things are thus possible and so nothing\\'s possible. This leads to postmodernistic nonsense, which is nothing less than an attempt to denigrate established truths so that all ideas, well-founded and stupid, are equal.<br /><br />To quote sci-fi writer Philip K. Dick, who put it so well, \"Reality is that which, when you stop believing in it, doesn\\'t go away.\"', 'Spoiler!! I love Branagh, love Helena Bonham-Carter, loved them together in \"Mary Shelley\\'s Frankenstein\" - but THIS -<br /><br />I can understand an actor\\'s desire to stretch, to avoid the romantic stereotype. Well, they did, but really - the script droned on, Bonham-Carter\\'s clothes were tres chic, and the occasional speeded-up \"madcap\" sequence could have been an outtake from a Beatles\\' movie, or the old Rowan and Martin Laugh-In.<br /><br />I never got the point - other commenters say the Branagh character was a dreamer. I never felt that. He was a loser, and not very bright, and certainly not endearing. The business with the bank robber disguise was merely painful to watch. Certainly not amusing.<br /><br />Bonham-Carter\\'s realistic (one supposes) attempts as realistic speech were harder to understand than the first 15 minutes of Lancashire accent in \"Full Monty.\"<br /><br />The poetic ending, with him high on a hill with her buried under the monstrosity of his airplane was too orchestrated. Was there a choir of angels, or merely a soundtrack?<br /><br />Go back to the classics or something with a spine and an arc to it. Donate this to PBS.<br /><br />'], 'labels': [0, 0]} {'texts': ['Being a gay man who lived through the time period examined in this tedious documentary, I was eager to see how the subject matter was handled. Unfortunately, the film makers wasted what could have been an energetic and insightful opportunity to shed some light on our collective gay history. This film only concerns itself with the period within New York City, ignoring the rest of the country. While I spent a fair amount of time in NYC at that time, I can assure you that there was a gay life outside Manhattan! The men interviewed here are the same \"A-list\" queens who thought they were better than anyone else during the 70s, and here they are again, waxing nostalgic and still throwing attitude. The film should have at least tried to cover larger topics, such as race, ageism, the burgeoning gay \"caste system\" based on wealth, body image, and the rise of the \"clones\", discrimination of sub-groups within the community, and the ability to grow a decent mustache (which was very important in the 70s!). Alas, we have none of this presented, and the recollections of those interviewed are no different than my own memories. If you were there in that decade, you\\'ll enjoy the archival photos and grainy home-movies of the bars and discos we haunted. If you weren\\'t there, this film will undoubtedly seem dull. It should have been so much more, but sadly, it\\'s not. Two stars for jogging my memory...I still miss going to the Anvil!', \"especially considering I can count on one hand the romantic comedy films I have ever enjoyed.<br /><br />Minnie Driver is very good as the heart transplant patient, who has a mysterious connection to Duchovny's recently deceased wife. (I can think of several awful films which have used this story line- I think there was an LMN movie with Jane Seymour) This film, however, is a keeper.<br /><br />Duchovny is sympathetic, and the scenes with his dog are cute and sad- the dog misses his deceased wife. All of his friends want him to find a replacement, and there is an amusing scene where he is on a blind date and Driver is the waitress. His date is horrible, and he finds himself intrigued by Minnie Driver.<br /><br />Caroll O'Connor is also good in one of his last roles, as the curmudgeonly grandfather. Bonnie Hunt and James Belushi (this is the only film I have liked him in) round out the comedy aspect of the film.<br /><br />This is a good film because the story works, it is not overly romantic, and does not insult the audience's intelligence. Highly recommended 9/10.\"], 'labels': [0, 1]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data_parallel(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.readline()\n",
    "\n",
    "def load_data(base_path):\n",
    "    paths = os.listdir(base_path)\n",
    "    result = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_path = {executor.submit(load_data_parallel, os.path.join(base_path, path)): path for path in paths}\n",
    "        for future in concurrent.futures.as_completed(future_to_path):\n",
    "            path = future_to_path[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                result.append(data)\n",
    "            except Exception as exc:\n",
    "                print(f\"Error reading {path}: {exc}\")\n",
    "    return result\n",
    "\n",
    "def get_dataset(base_path):\n",
    "    pos_data = load_data(os.path.join(base_path, 'pos'))\n",
    "    neg_data = load_data(os.path.join(base_path, 'neg'))\n",
    "\n",
    "    texts = pos_data + neg_data\n",
    "    labels = [[1]] * len(pos_data) + [[0]] * len(neg_data)\n",
    "    labels = torch.cat([torch.tensor(sublist) for sublist in labels])\n",
    "    labels = labels.type(torch.LongTensor)\n",
    "\n",
    "\n",
    "    dataset = Dataset.from_dict({'texts': texts, 'labels': labels})\n",
    "    shuffled_dataset = torch.utils.data.random_split(dataset, [len(dataset)])[0]\n",
    "    return shuffled_dataset\n",
    "\n",
    "test_base_path = 'D:/5555/aclImdb/test'  # 替换为你的数据文件夹路径\n",
    "test_dataset = get_dataset(test_base_path)\n",
    "\n",
    "train_base_path = 'D:/5555/aclImdb/train'  # 替换为你的数据文件夹路径\n",
    "train_dataset = get_dataset(train_base_path)\n",
    "\n",
    "print(train_dataset[0:2],test_dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28e0d04-9759-490d-b058-cf79708f7faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf14b098b664247a98afab0d7f7dc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871bda7567bc45b38d50475d50392587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "# Instantiate the configuration\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "\n",
    "# Instantiate the model\n",
    "pretrained = BertModel.from_pretrained(model_name, config=config).to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7411f7c2-826a-4a7c-84ea-e50e2168f1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,pretrained):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        # out = out.softmax(dim=1)\n",
    "        return out\n",
    "\n",
    "model=Model(pretrained)\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397e361b-0c0a-40d5-ba3c-b3e7828556f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.save_to_disk('./data/train_dataset')\n",
    "# test_dataset.save_to_disk('./data/test_dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac6214a-05ff-4e13-80e0-c2049d339b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ac463b03ca415e965372e06ecc6f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7ba9f533d74f79b6bfedeb8dd114c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295c7872926e4d9da0fc203ed35fd883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification,BertTokenizer,TrainingArguments,BertConfig,BertModel\n",
    "\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "token\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ec2b00-6d4d-4091-8994-2d964f110bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [item['texts'] for item in data]\n",
    "    labels = [item['labels'] for item in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                     batch_size=120,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d265ff6-e718-405e-8468-254a0657d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available with 1 device(s)!\n",
      "Device 0: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 检查CUDA是否可用\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    # 获取GPU设备的数量\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"CUDA is available with {device_count} device(s)!\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3826703d-9597-4980-999e-e78bddcd5864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Iteration 0, Loss: 0.6752526164054871, Accuracy: 0.6333333333333333\n",
      "Iteration 10, Loss: 0.6776291131973267, Accuracy: 0.6666666666666666\n",
      "Iteration 20, Loss: 0.6820762157440186, Accuracy: 0.5833333333333334\n",
      "Iteration 30, Loss: 0.6709997057914734, Accuracy: 0.65\n",
      "Iteration 40, Loss: 0.6770995259284973, Accuracy: 0.5916666666666667\n",
      "Iteration 50, Loss: 0.6685202121734619, Accuracy: 0.6833333333333333\n",
      "Iteration 60, Loss: 0.660487949848175, Accuracy: 0.6916666666666667\n",
      "Iteration 70, Loss: 0.6623037457466125, Accuracy: 0.7083333333333334\n",
      "Iteration 80, Loss: 0.6654340624809265, Accuracy: 0.65\n",
      "Iteration 90, Loss: 0.6594411730766296, Accuracy: 0.7166666666666667\n",
      "Iteration 100, Loss: 0.6651867628097534, Accuracy: 0.6166666666666667\n",
      "Iteration 110, Loss: 0.6611458659172058, Accuracy: 0.6916666666666667\n",
      "Iteration 120, Loss: 0.6537171602249146, Accuracy: 0.6666666666666666\n",
      "Iteration 130, Loss: 0.6561301350593567, Accuracy: 0.6916666666666667\n",
      "Iteration 140, Loss: 0.6437733173370361, Accuracy: 0.6833333333333333\n",
      "Iteration 150, Loss: 0.6311740279197693, Accuracy: 0.7666666666666667\n",
      "Iteration 160, Loss: 0.642667829990387, Accuracy: 0.675\n",
      "Iteration 170, Loss: 0.6255592703819275, Accuracy: 0.7833333333333333\n",
      "Iteration 180, Loss: 0.6382560729980469, Accuracy: 0.7166666666666667\n",
      "Iteration 190, Loss: 0.6293145418167114, Accuracy: 0.7916666666666666\n",
      "Iteration 200, Loss: 0.6319226026535034, Accuracy: 0.7166666666666667\n",
      "Epoch 2/10\n",
      "Iteration 0, Loss: 0.6330417394638062, Accuracy: 0.7416666666666667\n",
      "Iteration 10, Loss: 0.6086034774780273, Accuracy: 0.7833333333333333\n",
      "Iteration 20, Loss: 0.6426573395729065, Accuracy: 0.6916666666666667\n",
      "Iteration 30, Loss: 0.6332893371582031, Accuracy: 0.7083333333333334\n",
      "Iteration 40, Loss: 0.6296579241752625, Accuracy: 0.6583333333333333\n",
      "Iteration 50, Loss: 0.5993963479995728, Accuracy: 0.7583333333333333\n",
      "Iteration 60, Loss: 0.6012504696846008, Accuracy: 0.8\n",
      "Iteration 70, Loss: 0.6183418035507202, Accuracy: 0.7166666666666667\n",
      "Iteration 80, Loss: 0.5958144664764404, Accuracy: 0.7583333333333333\n",
      "Iteration 90, Loss: 0.5998592972755432, Accuracy: 0.7916666666666666\n",
      "Iteration 100, Loss: 0.6188981533050537, Accuracy: 0.7416666666666667\n",
      "Iteration 110, Loss: 0.6144856810569763, Accuracy: 0.725\n",
      "Iteration 120, Loss: 0.6109276413917542, Accuracy: 0.7166666666666667\n",
      "Iteration 130, Loss: 0.604485273361206, Accuracy: 0.7\n",
      "Iteration 140, Loss: 0.5981820821762085, Accuracy: 0.775\n",
      "Iteration 150, Loss: 0.6119450926780701, Accuracy: 0.6916666666666667\n",
      "Iteration 160, Loss: 0.5904664993286133, Accuracy: 0.7916666666666666\n",
      "Iteration 170, Loss: 0.5837200880050659, Accuracy: 0.7583333333333333\n",
      "Iteration 180, Loss: 0.597231924533844, Accuracy: 0.7416666666666667\n",
      "Iteration 190, Loss: 0.59731125831604, Accuracy: 0.7166666666666667\n",
      "Iteration 200, Loss: 0.5786986947059631, Accuracy: 0.7583333333333333\n",
      "Epoch 3/10\n",
      "Iteration 0, Loss: 0.5696691870689392, Accuracy: 0.7583333333333333\n",
      "Iteration 10, Loss: 0.6369485855102539, Accuracy: 0.65\n",
      "Iteration 20, Loss: 0.6325897574424744, Accuracy: 0.6583333333333333\n",
      "Iteration 30, Loss: 0.6193419694900513, Accuracy: 0.7333333333333333\n",
      "Iteration 40, Loss: 0.5989023447036743, Accuracy: 0.725\n",
      "Iteration 50, Loss: 0.5693085789680481, Accuracy: 0.7833333333333333\n",
      "Iteration 60, Loss: 0.5938153862953186, Accuracy: 0.725\n",
      "Iteration 70, Loss: 0.5680015683174133, Accuracy: 0.7583333333333333\n",
      "Iteration 80, Loss: 0.5854043960571289, Accuracy: 0.6833333333333333\n",
      "Iteration 90, Loss: 0.5898750424385071, Accuracy: 0.7583333333333333\n",
      "Iteration 100, Loss: 0.5941551327705383, Accuracy: 0.7333333333333333\n",
      "Iteration 110, Loss: 0.5789369940757751, Accuracy: 0.7333333333333333\n",
      "Iteration 120, Loss: 0.5584545135498047, Accuracy: 0.775\n",
      "Iteration 130, Loss: 0.6047996878623962, Accuracy: 0.7\n",
      "Iteration 140, Loss: 0.631495475769043, Accuracy: 0.7\n",
      "Iteration 150, Loss: 0.5751236081123352, Accuracy: 0.7583333333333333\n",
      "Iteration 160, Loss: 0.586119532585144, Accuracy: 0.7083333333333334\n",
      "Iteration 170, Loss: 0.6066223978996277, Accuracy: 0.6666666666666666\n",
      "Iteration 180, Loss: 0.5630602240562439, Accuracy: 0.75\n",
      "Iteration 190, Loss: 0.5685492753982544, Accuracy: 0.7333333333333333\n",
      "Iteration 200, Loss: 0.5874109268188477, Accuracy: 0.7583333333333333\n",
      "Epoch 4/10\n",
      "Iteration 0, Loss: 0.5703213810920715, Accuracy: 0.725\n",
      "Iteration 10, Loss: 0.6054579615592957, Accuracy: 0.6916666666666667\n",
      "Iteration 20, Loss: 0.6170504689216614, Accuracy: 0.65\n",
      "Iteration 30, Loss: 0.5296927094459534, Accuracy: 0.8333333333333334\n",
      "Iteration 40, Loss: 0.5291154384613037, Accuracy: 0.8166666666666667\n",
      "Iteration 50, Loss: 0.5548667311668396, Accuracy: 0.7666666666666667\n",
      "Iteration 60, Loss: 0.5389106869697571, Accuracy: 0.8083333333333333\n",
      "Iteration 70, Loss: 0.5596111416816711, Accuracy: 0.7416666666666667\n",
      "Iteration 80, Loss: 0.550746738910675, Accuracy: 0.7833333333333333\n",
      "Iteration 90, Loss: 0.5614031553268433, Accuracy: 0.725\n",
      "Iteration 100, Loss: 0.5692918300628662, Accuracy: 0.75\n",
      "Iteration 110, Loss: 0.5828367471694946, Accuracy: 0.7166666666666667\n",
      "Iteration 120, Loss: 0.5214006900787354, Accuracy: 0.8416666666666667\n",
      "Iteration 130, Loss: 0.6091654300689697, Accuracy: 0.65\n",
      "Iteration 140, Loss: 0.566278874874115, Accuracy: 0.7583333333333333\n",
      "Iteration 150, Loss: 0.5404561161994934, Accuracy: 0.8\n",
      "Iteration 160, Loss: 0.6076338291168213, Accuracy: 0.6916666666666667\n",
      "Iteration 170, Loss: 0.5737742781639099, Accuracy: 0.7083333333333334\n",
      "Iteration 180, Loss: 0.6114367246627808, Accuracy: 0.6583333333333333\n",
      "Iteration 190, Loss: 0.5475272536277771, Accuracy: 0.7916666666666666\n",
      "Iteration 200, Loss: 0.5152747631072998, Accuracy: 0.825\n",
      "Epoch 5/10\n",
      "Iteration 0, Loss: 0.5608685612678528, Accuracy: 0.7666666666666667\n",
      "Iteration 10, Loss: 0.5579036474227905, Accuracy: 0.75\n",
      "Iteration 20, Loss: 0.5164843797683716, Accuracy: 0.775\n",
      "Iteration 30, Loss: 0.5483783483505249, Accuracy: 0.7333333333333333\n",
      "Iteration 40, Loss: 0.546944260597229, Accuracy: 0.775\n",
      "Iteration 50, Loss: 0.5786150097846985, Accuracy: 0.6833333333333333\n",
      "Iteration 60, Loss: 0.5375166535377502, Accuracy: 0.75\n",
      "Iteration 70, Loss: 0.542518675327301, Accuracy: 0.8\n",
      "Iteration 80, Loss: 0.5560364127159119, Accuracy: 0.775\n",
      "Iteration 90, Loss: 0.5720527768135071, Accuracy: 0.7\n",
      "Iteration 100, Loss: 0.5483046174049377, Accuracy: 0.7666666666666667\n",
      "Iteration 110, Loss: 0.5393996834754944, Accuracy: 0.7916666666666666\n",
      "Iteration 120, Loss: 0.5357221364974976, Accuracy: 0.8083333333333333\n",
      "Iteration 130, Loss: 0.5540868639945984, Accuracy: 0.7583333333333333\n",
      "Iteration 140, Loss: 0.5334487557411194, Accuracy: 0.7916666666666666\n",
      "Iteration 150, Loss: 0.5176917314529419, Accuracy: 0.7416666666666667\n",
      "Iteration 160, Loss: 0.56231290102005, Accuracy: 0.7166666666666667\n",
      "Iteration 170, Loss: 0.5370202660560608, Accuracy: 0.775\n",
      "Iteration 180, Loss: 0.535455048084259, Accuracy: 0.7583333333333333\n",
      "Iteration 190, Loss: 0.52860426902771, Accuracy: 0.775\n",
      "Iteration 200, Loss: 0.5574322938919067, Accuracy: 0.725\n",
      "Epoch 6/10\n",
      "Iteration 0, Loss: 0.5457477569580078, Accuracy: 0.725\n",
      "Iteration 10, Loss: 0.5580114722251892, Accuracy: 0.7416666666666667\n",
      "Iteration 20, Loss: 0.5606668591499329, Accuracy: 0.7583333333333333\n",
      "Iteration 30, Loss: 0.5787470936775208, Accuracy: 0.7333333333333333\n",
      "Iteration 40, Loss: 0.5264427065849304, Accuracy: 0.7833333333333333\n",
      "Iteration 50, Loss: 0.5089163780212402, Accuracy: 0.8083333333333333\n",
      "Iteration 60, Loss: 0.5263679623603821, Accuracy: 0.725\n",
      "Iteration 70, Loss: 0.5154937505722046, Accuracy: 0.8083333333333333\n",
      "Iteration 80, Loss: 0.5258151292800903, Accuracy: 0.8\n",
      "Iteration 90, Loss: 0.5099782347679138, Accuracy: 0.7916666666666666\n",
      "Iteration 100, Loss: 0.46494612097740173, Accuracy: 0.8666666666666667\n",
      "Iteration 110, Loss: 0.5519282817840576, Accuracy: 0.7333333333333333\n",
      "Iteration 120, Loss: 0.542806088924408, Accuracy: 0.775\n",
      "Iteration 130, Loss: 0.539492666721344, Accuracy: 0.7916666666666666\n",
      "Iteration 140, Loss: 0.5074487328529358, Accuracy: 0.8166666666666667\n",
      "Iteration 150, Loss: 0.5434818863868713, Accuracy: 0.7666666666666667\n",
      "Iteration 160, Loss: 0.4900287091732025, Accuracy: 0.8166666666666667\n",
      "Iteration 170, Loss: 0.48215821385383606, Accuracy: 0.8166666666666667\n",
      "Iteration 180, Loss: 0.524219274520874, Accuracy: 0.7583333333333333\n",
      "Iteration 190, Loss: 0.5306548476219177, Accuracy: 0.75\n",
      "Iteration 200, Loss: 0.5294617414474487, Accuracy: 0.7666666666666667\n",
      "Epoch 7/10\n",
      "Iteration 0, Loss: 0.5307552814483643, Accuracy: 0.7583333333333333\n",
      "Iteration 10, Loss: 0.49011117219924927, Accuracy: 0.7833333333333333\n",
      "Iteration 20, Loss: 0.5390385389328003, Accuracy: 0.7416666666666667\n",
      "Iteration 30, Loss: 0.5465456247329712, Accuracy: 0.7666666666666667\n",
      "Iteration 40, Loss: 0.49667614698410034, Accuracy: 0.825\n",
      "Iteration 50, Loss: 0.5241165161132812, Accuracy: 0.7916666666666666\n",
      "Iteration 60, Loss: 0.49033021926879883, Accuracy: 0.775\n",
      "Iteration 70, Loss: 0.48184338212013245, Accuracy: 0.825\n",
      "Iteration 80, Loss: 0.563338577747345, Accuracy: 0.7333333333333333\n",
      "Iteration 90, Loss: 0.4871208071708679, Accuracy: 0.8166666666666667\n",
      "Iteration 100, Loss: 0.5016668438911438, Accuracy: 0.775\n",
      "Iteration 110, Loss: 0.49364280700683594, Accuracy: 0.8083333333333333\n",
      "Iteration 120, Loss: 0.527418851852417, Accuracy: 0.7666666666666667\n",
      "Iteration 130, Loss: 0.5170424580574036, Accuracy: 0.7833333333333333\n",
      "Iteration 140, Loss: 0.515845537185669, Accuracy: 0.7583333333333333\n",
      "Iteration 150, Loss: 0.5612583160400391, Accuracy: 0.7583333333333333\n",
      "Iteration 160, Loss: 0.4864856004714966, Accuracy: 0.8083333333333333\n",
      "Iteration 170, Loss: 0.4789417088031769, Accuracy: 0.8166666666666667\n",
      "Iteration 180, Loss: 0.5053993463516235, Accuracy: 0.825\n",
      "Iteration 190, Loss: 0.5505905151367188, Accuracy: 0.7333333333333333\n",
      "Iteration 200, Loss: 0.5191828012466431, Accuracy: 0.7833333333333333\n",
      "Epoch 8/10\n",
      "Iteration 0, Loss: 0.5185521841049194, Accuracy: 0.7416666666666667\n",
      "Iteration 10, Loss: 0.5577700138092041, Accuracy: 0.7\n",
      "Iteration 20, Loss: 0.47960764169692993, Accuracy: 0.825\n",
      "Iteration 30, Loss: 0.5881063938140869, Accuracy: 0.6416666666666667\n",
      "Iteration 40, Loss: 0.5108353495597839, Accuracy: 0.775\n",
      "Iteration 50, Loss: 0.5078461170196533, Accuracy: 0.7916666666666666\n",
      "Iteration 60, Loss: 0.5078662633895874, Accuracy: 0.7583333333333333\n",
      "Iteration 70, Loss: 0.5472499132156372, Accuracy: 0.7083333333333334\n",
      "Iteration 80, Loss: 0.48670944571495056, Accuracy: 0.8\n",
      "Iteration 90, Loss: 0.5048189759254456, Accuracy: 0.7583333333333333\n",
      "Iteration 100, Loss: 0.5188592076301575, Accuracy: 0.7666666666666667\n",
      "Iteration 110, Loss: 0.5646394491195679, Accuracy: 0.725\n",
      "Iteration 120, Loss: 0.4914245903491974, Accuracy: 0.75\n",
      "Iteration 130, Loss: 0.5159353017807007, Accuracy: 0.7666666666666667\n",
      "Iteration 140, Loss: 0.539275586605072, Accuracy: 0.7333333333333333\n",
      "Iteration 150, Loss: 0.5226839780807495, Accuracy: 0.7583333333333333\n",
      "Iteration 160, Loss: 0.4864315688610077, Accuracy: 0.7916666666666666\n",
      "Iteration 170, Loss: 0.533247172832489, Accuracy: 0.7666666666666667\n",
      "Iteration 180, Loss: 0.444491446018219, Accuracy: 0.8833333333333333\n",
      "Iteration 190, Loss: 0.5026542544364929, Accuracy: 0.7833333333333333\n",
      "Iteration 200, Loss: 0.48774421215057373, Accuracy: 0.7833333333333333\n",
      "Epoch 9/10\n",
      "Iteration 0, Loss: 0.5082007646560669, Accuracy: 0.7583333333333333\n",
      "Iteration 10, Loss: 0.5357276797294617, Accuracy: 0.7416666666666667\n",
      "Iteration 20, Loss: 0.5230942368507385, Accuracy: 0.7833333333333333\n",
      "Iteration 30, Loss: 0.46858376264572144, Accuracy: 0.8\n",
      "Iteration 40, Loss: 0.5157313942909241, Accuracy: 0.7583333333333333\n",
      "Iteration 50, Loss: 0.5284146666526794, Accuracy: 0.7166666666666667\n",
      "Iteration 60, Loss: 0.5498266220092773, Accuracy: 0.75\n",
      "Iteration 70, Loss: 0.5195887088775635, Accuracy: 0.7666666666666667\n",
      "Iteration 80, Loss: 0.5043043494224548, Accuracy: 0.7333333333333333\n",
      "Iteration 90, Loss: 0.5658569931983948, Accuracy: 0.725\n",
      "Iteration 100, Loss: 0.5110946297645569, Accuracy: 0.775\n",
      "Iteration 110, Loss: 0.507938027381897, Accuracy: 0.775\n",
      "Iteration 120, Loss: 0.4848414361476898, Accuracy: 0.7833333333333333\n",
      "Iteration 130, Loss: 0.48381736874580383, Accuracy: 0.8166666666666667\n",
      "Iteration 140, Loss: 0.5570593476295471, Accuracy: 0.7083333333333334\n",
      "Iteration 150, Loss: 0.5044016242027283, Accuracy: 0.7916666666666666\n",
      "Iteration 160, Loss: 0.5231650471687317, Accuracy: 0.7583333333333333\n",
      "Iteration 170, Loss: 0.5695350766181946, Accuracy: 0.7\n",
      "Iteration 180, Loss: 0.5591003894805908, Accuracy: 0.7416666666666667\n",
      "Iteration 190, Loss: 0.5456941723823547, Accuracy: 0.7666666666666667\n",
      "Iteration 200, Loss: 0.49574536085128784, Accuracy: 0.8\n",
      "Epoch 10/10\n",
      "Iteration 0, Loss: 0.49739882349967957, Accuracy: 0.8166666666666667\n",
      "Iteration 10, Loss: 0.499154657125473, Accuracy: 0.7916666666666666\n",
      "Iteration 20, Loss: 0.462858647108078, Accuracy: 0.7833333333333333\n",
      "Iteration 30, Loss: 0.4837471544742584, Accuracy: 0.8083333333333333\n",
      "Iteration 40, Loss: 0.5322718024253845, Accuracy: 0.75\n",
      "Iteration 50, Loss: 0.47934702038764954, Accuracy: 0.7916666666666666\n",
      "Iteration 60, Loss: 0.5132110118865967, Accuracy: 0.725\n",
      "Iteration 70, Loss: 0.4781707227230072, Accuracy: 0.775\n",
      "Iteration 80, Loss: 0.5593439340591431, Accuracy: 0.7333333333333333\n",
      "Iteration 90, Loss: 0.5230969190597534, Accuracy: 0.7583333333333333\n",
      "Iteration 100, Loss: 0.5391746163368225, Accuracy: 0.7416666666666667\n",
      "Iteration 110, Loss: 0.5324385166168213, Accuracy: 0.75\n",
      "Iteration 120, Loss: 0.4848831295967102, Accuracy: 0.8333333333333334\n",
      "Iteration 130, Loss: 0.5160340070724487, Accuracy: 0.775\n",
      "Iteration 140, Loss: 0.5859248042106628, Accuracy: 0.7166666666666667\n",
      "Iteration 150, Loss: 0.4961848855018616, Accuracy: 0.8166666666666667\n",
      "Iteration 160, Loss: 0.413595587015152, Accuracy: 0.875\n",
      "Iteration 170, Loss: 0.5304993987083435, Accuracy: 0.75\n",
      "Iteration 180, Loss: 0.5223826169967651, Accuracy: 0.7416666666666667\n",
      "Iteration 190, Loss: 0.5146161317825317, Accuracy: 0.7666666666666667\n",
      "Iteration 200, Loss: 0.5158011317253113, Accuracy: 0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.00005)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(\"cuda:0\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 10  # 设置训练的轮数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "        input_ids = input_ids.to(\"cuda:0\")\n",
    "        attention_mask = attention_mask.to(\"cuda:0\")\n",
    "        token_type_ids = token_type_ids.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:0\")\n",
    "\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            preds = out.argmax(dim=1)\n",
    "            accuracy = (preds == labels).sum().item() / len(labels)\n",
    "            print(f\"Iteration {i}, Loss: {loss.item()}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a423e6b8-1289-4d88-98fa-a0c4ab468a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77125\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 100:\n",
    "            break\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_ids = input_ids.to(\"cuda:0\")\n",
    "            attention_mask = attention_mask.to(\"cuda:0\")\n",
    "            token_type_ids = token_type_ids.to(\"cuda:0\")\n",
    "            labels = labels.to(\"cuda:0\")\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6771fbd-9ab6-4b1d-a059-74fa5d5b0a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
